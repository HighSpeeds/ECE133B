\documentclass[10pt]{article}
\usepackage{geometry}
\geometry{a4paper, portrait, margin=1in}
\author{Lawrence Liu}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pdfpages}
\newcommand{\Laplace}{\mathscr{L}}
\DeclareMathOperator{\Tr}{tr}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%
\usepackage{xcolor}
\usepackage{listings}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usepackage{amssymb}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour}}
\lstset{style=mystyle}

\begin{document}
We have that the estimated density from the kernel density estimator is given by
\begin{equation}
\hat{f}_{\theta}(x) = \frac{1}{n} \sum_{i=1}^n K_{\theta}(x-x_i')
\end{equation}
where $K_h$ is the kernel function, and $\theta$ are the parameters, and $x_i'$ is the $i$th training point. 
In our case $x_i'$ is a $m$ dimensional vector representing the daily change for each of the $m$ stocks in our dataset.
We assume that that each kernel function is normalized $\int_{\mathbb{R}^d} K_h(x) dx = 1$
\section*{Optimizing the Kernel Parameters}
We would want to minimize the weighted log likelihood function, ie we would want to minimize:
\begin{equation}
    \mathcal{L}(x_1',\dots,x_k') = -\sum_{i=1}^k w_i\log(\hat{f}_{\theta}(x_i'))
\end{equation}
Where are $w_i$ are weights of the $i$th training point, we will experiments with different weighting mechanisms later on.
\subsection*{Gaussian Kernel}
Let us consider the case of a gaussian kernel 
$$K_{\Sigma}(x) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}}\exp\left(-\frac{1}{2}x^T\Sigma^{-1}x\right)$$
Where $\Sigma$ is the covariance matrix for the kernel. Because the 
covariance matrix is symmetric positive semidefinite 
we can express $\Sigma$ as $\Sigma = R^TR$ through Cholesky decomposition.
We have that the derivative of the 
weighted log likelihood function is given by:
\begin{align*}
    \frac{\partial}{\partial R}\mathcal{L}(x_1',\dots,x_k') &= -\sum_{i=1}^k w_i \frac{1}{\hat{f}_{\theta}(x_i')} \frac{\partial \hat{f}_{\theta}(x_i')}{\partial R}
\end{align*}
We have that 
\begin{align*}
    \frac{\partial |\Sigma|}{\partial R} &= \frac{\partial |R^TR|}{\partial R} \\
    &= 2|\Sigma|R^{-T}
\end{align*}
Therefore we have that 
\begin{equation}
    \frac{\partial}{\partial R}\frac{1}{\sqrt{(2\pi)^d|\Sigma|}} = -R^{-T}\frac{1}{\sqrt{(2\pi)^d|\Sigma|}}
\end{equation}
%     \frac{\partial}{\partial R}\frac{1}{\sqrt{(2\pi)^d|\Sigma|}} &= -R^{-T}\frac{1}{\sqrt{(2\pi)^d}}
% \end{align*}
We also have that:
\begin{equation}
    \frac{\partial}{\partial R} e^{\frac{1}{2}x^T\Sigma^{-1}x} = \frac{1}{2}e^{\frac{1}{2}x^T\Sigma^{-1}x} \frac{\partial}{\partial R} x^T(R^{-1}R^{-T})x
\end{equation}
$\frac{\partial}{\partial R} x^T(R^{-1}R^{-T})x$ is very difficult to calculate, so we must approximate it. First we note that for 
a function $f(\mathbf{x})$ that takes in a vector $\mathbf{x}$ we have that the first order taylor expansion is given by:
\begin{equation}
    f(\mathbf{x}+\Delta \mathbf{x}) \approx f(\mathbf{x}) + \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}\Delta \mathbf{x}
\end{equation}
Where $\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}$ is a $1\times d$ vector, if $\mathbf{x}$ is a $d$ dimensional vector. Therefore 
we argue that a generalization to a function of a matrix $\mathbf{X}$ is given by:
\begin{equation}
    f(\mathbf{X}+\Delta \mathbf{X}) \approx f(\mathbf{X}) + \mathbf{1}^T\left(\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}}\circ \Delta \mathbf{X}\right)\mathbf{1}
\end{equation}
Where $\circ$ is the Hadamard product, and $\mathbf{1}$ is a $d\times 1$ vector of ones. We note that $\mathbf{1}^T\left(\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}}\circ \Delta \mathbf{X}\right)\mathbf{1}$
equals to $\Tr\left(\left(\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}}\right)^T\mathbf{X}\right)$. We have for our specific case:
\begin{align*}
    x^T((R+\delta R)^{-1}(R+\delta R)^{-T})x &\approx x^T(R^{-1}R^{-T})x + \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)
\end{align*}
We note that for small pertubations, $(R+\delta R)^{-1} \approx R^{-1} - R^{-1}\delta R R^{-1}$, and therefore:
\begin{align*}
    x^T (R^{-1} - R^{-1}\delta R R^{-1})(R^{-T} - R^{-T}\delta R^T R^{-T})x &\approx x^T(R^{-1}R^{-T})x + \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)
\end{align*}
Only keeping the zeroth order and first order terms, we have that:
\begin{align*}
    x^T(R^{-1}R^{-T})x - x^T \left(R^{-1}\delta R R^{-1}R^{-T} + R^{-1}R^{-T}\delta R^T R^{-T}\right)x \approx& x^T(R^{-1}R^{-T})x \\&+ \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)\\
\end{align*}
\begin{equation}
    - x^T \left(R^{-1}\delta R R^{-1}R^{-T} + R^{-1}R^{-T}\delta R^T R^{-T}\right)x \approx \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)\\
\end{equation}
Because the left side is a scalar, we can apply an trace operator to both sides, and noting that $R^{-1}R^{-T}=\Sigma^{-1}$, we have that:
\begin{align*}
    -\Tr\left(x^T \left(R^{-1}\delta R \Sigma^{-1} + \Sigma^{-1}\delta R^T R^{-T}\right)x\right) &\approx \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)\\
    -\Tr\left(x^T R^{-1}\delta R \Sigma^{-1}x\right) -\Tr\left( x^T\Sigma^{-1}\delta R^T R^{-T}x\right)&\approx\\
    -\Tr\left(x^T R^{-1}\delta R \Sigma^{-1}x\right)-\Tr\left( x^TR^{-1}\delta R \Sigma^{-T} x\right)&\approx\\
    -2\Tr\left(x^T R^{-1}\delta R \Sigma^{-1}x\right) &\approx\\
    -2\Tr\left( \Sigma^{-1}xx^T R^{-1}\delta R\right) &\approx \Tr\left(\left(\frac{\partial(x^TR^{-1}R^{-T})x}{\partial R}\right)^T  \delta R\right)
\end{align*}
Therefore we can see that 
\begin{equation}
    \frac{\partial}{\partial R} x^T(R^{-1}R^{-T})x \approx -2\Sigma^{-1}xx^T R^{-1}
\end{equation}
Therefore we have that:
\begin{equation}
    \frac{\partial}{\partial R} e^{\frac{1}{2}x^T\Sigma^{-1}x} \approx -e^{\frac{1}{2}x^T\Sigma^{-1}x} \Sigma^{-1}xx^T R^{-1}
\end{equation}
Therefore we have that
\begin{equation*}
    \frac{\partial}{\partial R} K_{\Sigma}(x) = -K_{\Sigma}(x)R^{-T} - K_{\Sigma}(x)\Sigma^{-1}xx^T R^{-1}
\end{equation*} 



\end{document}